{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Theory\n",
    "## 1.1\n",
    "### Deep learning vs. shallow learning\n",
    "\n",
    "Deep learning is characterized as representational learning. This means that the data samples supported to the learning system is an a raw format. This could be a matrix of image pixels.\n",
    "\n",
    "These inputs are then passed throug a multi-layered network which can pick up on many complex features. An example would be a convulational network made for classifing animals.\n",
    "\n",
    "Shallow learning, on the other side, is based on the user having some prior knowledge of the specific features on the input data. Feature selection and optimization is often emphasized when it comes to shallow learning.\n",
    "\n",
    "An example of shallow learning is a neural network which solves the XOR problem. Here, we only need one layer, and the input is not exactly in a \"raw\" format.\n",
    "\n",
    "\n",
    "## 1.2\n",
    "In this section, I will look at some of the advantages and disadantanges of a selection of machine learning methods. The \"No Free Lunch\" theorem states that there is not one model that works best for every problem, as we will see.\n",
    "\n",
    "#### k-NN\n",
    "- k-nearest neighbours is a easy and simple machine learning model\n",
    "- Use for classification or regression. By using the mean value of the k-nearest values, one can use it as a regression mode. By using \"votes\" one can determine as to which class the unknown data sample belongs to.\n",
    "- This model works only if the data is graphable\n",
    "\n",
    "![Image of Yaktocat](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/03faee64-e85e-4ea0-a2b4-e5964949e2d1/d99b9a4d-618c-45f0-86d1-388bdf852c1d/images/screenshot.gif)\n",
    "\n",
    "\n",
    "k-NN is computationally costly, meaning it is not suitable if the data set is large.\n",
    "\n",
    "#### Decision tree\n",
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/1200/1*XMId5sJqPtm8-RIwVVz2tg.png\n",
    ")\n",
    "- Supports non-linearity\n",
    "- Not so suitable if there are many features with smaller data set\n",
    "- Prone to outliers\n",
    "- Decision trees are \"white box\". Compared to Deep Neural Networks, where the hidden layers are difficult to reason with, it is easier to determine why a data point given a certain classification.\n",
    "\n",
    "Decision trees do not work that well on small data sets with many features.\n",
    "\n",
    "\n",
    "#### SVM\n",
    "![Image of Yaktocat](https://www.researchgate.net/publication/309361744/figure/fig3/AS:614002350620688@1523400976304/Support-vector-machine-SVM-classifier.png\n",
    ")\n",
    "https://www.researchgate.net/publication/309361744/figure/fig3/AS:614002350620688@1523400976304/Support-vector-machine-SVM-classifier.png\n",
    "- SVM supports both linear and non-linear \n",
    "\n",
    "#### Deep learning\n",
    "- Suitable when there is a lot of data\n",
    "\n",
    "## 1.3\n",
    "### Ensamble methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
