{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=LsK-xG1cLYA\n",
    "# https://i.stack.imgur.com/CTpi9.png\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset(3)/adaboost_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0    -0.378117 -0.190990 -1.256752 -1.680321 -0.284953 -0.197824 -1.839744   \n",
      "1     0.327182  1.277672  0.074589  0.836706  2.288972 -0.017284 -0.678337   \n",
      "2    -0.630357 -2.168994  0.431652 -1.139658  0.080044 -0.004626 -0.279533   \n",
      "3    -0.219106  2.627749 -0.039370 -0.121920  0.524167  1.726919  0.254259   \n",
      "4    -0.447902 -1.243640 -0.885404  0.466631 -0.465717  0.343594 -1.093293   \n",
      "5     0.488004 -0.540104  1.526822  0.714374  0.546372 -0.180352  1.101560   \n",
      "6     0.007572 -0.794606  1.579415  0.348249  1.116504 -0.875715 -0.666563   \n",
      "7     1.474398  0.429162 -1.282847  1.650941  0.067787 -1.074453  0.994767   \n",
      "8     0.626374  1.077572 -0.734284 -0.362344  1.225062 -0.582762 -0.135598   \n",
      "9    -0.111202  0.368287 -0.390027 -0.790413  1.445323  1.565810 -0.066750   \n",
      "10    1.127285  0.529622 -1.195184  0.770720  1.665169  0.683614 -0.256241   \n",
      "11   -0.382433  1.683779 -0.049091 -0.475609  0.678207  0.101162  0.854233   \n",
      "12    0.362051 -0.018643  0.062644  1.257039  0.161914 -1.906539 -0.184904   \n",
      "13    0.062031 -1.459810 -1.005162 -0.726235  0.233353 -0.035301 -0.926387   \n",
      "14    1.093317 -0.220093  0.522093 -0.130887 -1.871309 -0.877313 -0.877169   \n",
      "15    0.138261  0.456262 -1.271692 -0.925442  0.438945 -0.111234 -1.640295   \n",
      "16   -0.483080  0.241289  1.464155  0.368645 -1.183562  0.055148  1.420980   \n",
      "17    0.549596 -1.978764  0.465842 -0.799845  0.686698 -0.504929 -0.795147   \n",
      "18    1.010053  1.990793  1.063267 -1.059915 -1.121309  0.273320 -0.401959   \n",
      "19   -0.274014  1.295931  1.024218 -0.380588 -0.702148 -1.695351 -1.398099   \n",
      "20   -0.582732 -0.464635 -0.508872  1.756239 -0.162616  0.221419  0.845387   \n",
      "21    0.475381  1.346122 -0.988398  0.147980 -0.068643  0.288325 -0.988088   \n",
      "22    0.246750  0.591776 -1.584090  0.015854  0.595925 -0.747300  1.291843   \n",
      "23    1.369514  1.108241  1.646117  0.359674  0.804113 -1.912516 -1.536171   \n",
      "24   -0.192628 -0.165786  0.047877  0.400958  0.211317 -1.865292  0.405813   \n",
      "25   -0.087005 -2.538027 -0.366847  1.339896 -0.807135 -0.236976 -1.471858   \n",
      "26    0.085351 -0.557991 -0.033849 -0.376032  0.605025  0.250946  1.661772   \n",
      "27    1.359971 -2.541266 -0.374153 -0.871653 -1.000436 -0.031138  1.962079   \n",
      "28   -1.214906  0.438764  0.101242 -0.079197  0.934505 -0.273295 -1.280107   \n",
      "29    1.120988  1.274384  0.712622  0.094302  0.255220  1.211899 -0.219773   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9570  0.008310 -1.041680 -1.092490 -0.518253 -0.643507  0.539388  0.368200   \n",
      "9571  1.993700  0.739192 -1.674409  0.440689  0.716932 -0.122305 -0.867814   \n",
      "9572  1.215646 -1.194475  1.897891 -1.291048 -0.386838  2.385907  0.028831   \n",
      "9573  0.757258  1.257218 -0.593575  0.657917 -2.238126 -0.658017  0.846000   \n",
      "9574 -1.181769 -0.120989 -0.390192 -0.004346  0.048804  0.025087  0.254131   \n",
      "9575 -2.719339  0.259566 -0.382938  1.127803 -1.377514  1.686086 -0.767142   \n",
      "9576 -1.285349  0.659254  0.644572  0.363706  0.926906 -0.186881  0.809181   \n",
      "9577 -1.822535  1.173643 -1.678331 -0.593297 -1.257771  0.316063  1.949678   \n",
      "9578 -1.485861  0.845645 -1.614837  0.382605 -0.448760 -0.980215  0.314550   \n",
      "9579 -1.320601 -0.151359  0.848515  1.904641  0.244239 -0.289926  0.859951   \n",
      "9580 -1.791875  1.426973  0.133254 -0.865909 -0.612812 -0.375683  0.884285   \n",
      "9581 -2.449422  0.371885 -0.187419  0.008535  0.846370 -1.216116 -1.433710   \n",
      "9582  0.292737  0.671970  1.012063 -0.700505  0.613355 -0.138023 -0.705495   \n",
      "9583  1.073710  0.919912 -0.641320  0.457132 -0.557910 -1.016501 -0.494623   \n",
      "9584  0.120157 -1.002680  0.716427  1.249401  0.332391 -1.173736 -1.855986   \n",
      "9585  0.213045 -2.145134 -1.758789  0.486519 -0.740057  1.046883  0.685487   \n",
      "9586 -0.922038  0.271025 -0.375572  0.202177 -0.899872  1.442868  1.279517   \n",
      "9587  1.064813 -0.098490  1.127242  0.229775  0.211993  0.979358 -0.319281   \n",
      "9588 -1.040190  0.058142 -0.376638 -0.746579 -0.935251 -1.368250 -0.582495   \n",
      "9589  0.715630 -1.128506 -0.094811 -1.088569  1.104272  0.130448 -0.096804   \n",
      "9590  1.969324 -0.900541  0.684816 -0.273189  0.509227 -0.980360  0.941464   \n",
      "9591 -0.355882  0.374550  1.800323 -0.569226  0.963309  0.670335 -0.754701   \n",
      "9592  0.436190  0.968944 -0.330017  0.581854 -0.448633 -1.062361  0.606181   \n",
      "9593 -0.233756 -0.509410 -0.385725  1.170572  0.985919  0.506015 -0.053593   \n",
      "9594  0.317510  0.049646  0.709094 -0.811677  0.822703 -0.807813  0.303234   \n",
      "9595 -0.644230 -0.163113  0.363038  0.880563  0.237142 -1.246870 -0.568435   \n",
      "9596  0.866527  1.011097  1.064874 -0.346888 -1.036108 -1.780311  0.934477   \n",
      "9597 -0.516058 -1.487840  0.741749  0.120402  0.392928 -0.147215  1.292459   \n",
      "9598  0.328214 -0.432331  0.003530 -0.411207  1.663729 -0.479392 -0.514922   \n",
      "9599 -0.906642  1.438092  0.113375 -0.277652 -0.370702  1.575345 -1.077036   \n",
      "\n",
      "            x8        x9       x10  \n",
      "0    -1.502530  0.522157  0.826696  \n",
      "1     2.088189 -0.967867 -1.547200  \n",
      "2     0.580813 -0.434184 -1.545773  \n",
      "3    -0.187440  0.771588 -0.171752  \n",
      "4    -0.514961  0.628765  0.189104  \n",
      "5     0.131636 -0.674342  0.620319  \n",
      "6     0.194058 -0.755015  0.878282  \n",
      "7     1.314694 -0.778351 -1.378355  \n",
      "8     0.757520  0.656781  0.542430  \n",
      "9    -0.454278  1.777026  1.580263  \n",
      "10   -0.034839  1.582467  1.178948  \n",
      "11    0.357141  0.663535 -0.704811  \n",
      "12    0.859371  0.473393  0.900719  \n",
      "13   -0.362508  0.615345 -0.646786  \n",
      "14   -1.690905  0.368504 -0.330575  \n",
      "15    0.548702  1.144443 -0.321384  \n",
      "16    0.176653 -2.556045  0.411867  \n",
      "17   -0.413797 -0.568084  0.837328  \n",
      "18   -0.120684  0.804890  1.118983  \n",
      "19    0.214874 -0.736335  1.106044  \n",
      "20    0.785103  0.305153  0.711978  \n",
      "21    0.534606  1.130667  0.012549  \n",
      "22   -0.654570 -1.519381 -0.894916  \n",
      "23    0.535993 -0.181373  0.131865  \n",
      "24    0.149891 -0.200105  0.730721  \n",
      "25   -1.357937 -0.306472 -0.944233  \n",
      "26    0.447383 -0.895537 -1.455415  \n",
      "27   -0.885474 -1.601782  0.044578  \n",
      "28    2.026683  1.460358  0.113052  \n",
      "29   -0.221301 -1.557048  0.582349  \n",
      "...        ...       ...       ...  \n",
      "9570  0.634833  0.607318  0.162843  \n",
      "9571  0.501875  0.151269  0.228056  \n",
      "9572 -0.643667 -2.192796 -0.246395  \n",
      "9573  0.234658  1.195222  0.905392  \n",
      "9574 -0.125950  1.500553  1.063500  \n",
      "9575 -1.024237  0.875583  0.943606  \n",
      "9576 -0.657115  0.057416 -0.200684  \n",
      "9577 -0.004232 -1.473929  0.326721  \n",
      "9578  0.711894 -0.544014 -1.011472  \n",
      "9579  0.499182 -0.548427  0.497766  \n",
      "9580 -1.225526  0.711127  0.308954  \n",
      "9581 -0.510959 -1.283187 -1.040038  \n",
      "9582  0.046825  0.023622 -0.588472  \n",
      "9583  1.941879  1.052120  1.079227  \n",
      "9584  0.518364 -1.201220  0.718713  \n",
      "9585 -0.605416  0.728652  1.400317  \n",
      "9586  0.816827 -0.565330 -1.391310  \n",
      "9587 -0.442097 -0.004217 -0.259757  \n",
      "9588 -0.893868  0.472697  0.551776  \n",
      "9589  0.903870 -1.476128  0.474610  \n",
      "9590 -0.631382  0.539393  0.516958  \n",
      "9591  0.283787 -0.927318 -1.372299  \n",
      "9592 -0.338648  0.534311  0.155561  \n",
      "9593 -0.954533 -1.246466  0.438361  \n",
      "9594 -0.037439 -1.518118  0.278268  \n",
      "9595  0.881509 -2.371444  0.296561  \n",
      "9596  0.591367 -0.058095 -0.316957  \n",
      "9597 -0.647950 -0.065279  0.520018  \n",
      "9598 -1.213083 -0.203498  0.241337  \n",
      "9599 -1.758538  0.125502  0.880881  \n",
      "\n",
      "[9600 rows x 10 columns]\n",
      "        y\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4    -1.0\n",
      "5    -1.0\n",
      "6    -1.0\n",
      "7     1.0\n",
      "8    -1.0\n",
      "9     1.0\n",
      "10    1.0\n",
      "11   -1.0\n",
      "12   -1.0\n",
      "13   -1.0\n",
      "14    1.0\n",
      "15   -1.0\n",
      "16    1.0\n",
      "17   -1.0\n",
      "18    1.0\n",
      "19    1.0\n",
      "20   -1.0\n",
      "21   -1.0\n",
      "22   -1.0\n",
      "23    1.0\n",
      "24   -1.0\n",
      "25    1.0\n",
      "26   -1.0\n",
      "27    1.0\n",
      "28    1.0\n",
      "29   -1.0\n",
      "...   ...\n",
      "9570 -1.0\n",
      "9571 -1.0\n",
      "9572  1.0\n",
      "9573  1.0\n",
      "9574 -1.0\n",
      "9575  1.0\n",
      "9576 -1.0\n",
      "9577  1.0\n",
      "9578 -1.0\n",
      "9579 -1.0\n",
      "9580  1.0\n",
      "9581  1.0\n",
      "9582 -1.0\n",
      "9583  1.0\n",
      "9584  1.0\n",
      "9585  1.0\n",
      "9586 -1.0\n",
      "9587 -1.0\n",
      "9588 -1.0\n",
      "9589 -1.0\n",
      "9590 -1.0\n",
      "9591 -1.0\n",
      "9592 -1.0\n",
      "9593 -1.0\n",
      "9594 -1.0\n",
      "9595  1.0\n",
      "9596 -1.0\n",
      "9597 -1.0\n",
      "9598 -1.0\n",
      "9599  1.0\n",
      "\n",
      "[9600 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train = df.loc[:,'x1':'x10']\n",
    "Y_train = df[['y']]\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# clf = tree.DecisionTreeClassifier(max_depth = 1)\n",
    "# clf = clf.fit(X_train,Y_train)\n",
    "# clf\n",
    "# import graphviz\n",
    "# dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "# graph = graphviz.Source(dot_data) \n",
    "# graph\n",
    "\n",
    "# print(clf.predict([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoost(X_train,Y_train, clf2):\n",
    "    classifiers = []\n",
    "    # initializing the weights:\n",
    "    N = len(Y_train)\n",
    "    # w_i = [1 / N] * N\n",
    "    w_i = np.ones(N)/N\n",
    "\n",
    "    T = 100\n",
    "    x_train = (X_train.apply(lambda x: x.tolist(), axis=1))\n",
    "    clf_errors = []\n",
    "\n",
    "    for t in range(T):\n",
    "        print(\"Iteration:\", t)\n",
    "        print(\"WEUGHTS SUM\")\n",
    "        print(sum(w_i))\n",
    "        # clf = clf2.fit(X_train,Y_train, sample_weight = w_i)\n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "        clf.fit(x_train, y_train, sample_weight = w_i)\n",
    "        \n",
    "        #Predict all the values:\n",
    "        y_pred = []\n",
    "        for sample in x_train:\n",
    "            p = clf.predict([sample])\n",
    "            p = p[0]\n",
    "            y_pred.append(p)\n",
    "        print(\" \")\n",
    "        print(\"HHHEKKKO\", sum(y_pred))\n",
    "        num_of_incorrect = calculate_error_clf(y_pred, Y_train)\n",
    "        print(\"num_of_incorrect\", num_of_incorrect)\n",
    "        \n",
    "        \n",
    "        clf_errors.append(num_of_incorrect)\n",
    "        \n",
    "        error_internal = calc_error(w_i,Y_train,y_pred)\n",
    "        \n",
    "        alpha = np.log((1-error_internal)/ error_internal)\n",
    "        print(alpha)\n",
    "        \n",
    "        # Add the predictions, error and alpha for later use for every iteration\n",
    "        classifiers.append((y_pred, error_internal, alpha))\n",
    "        \n",
    "        if t == 2 and y_pred == classifiers[0][0]:\n",
    "            print(\"TRUE\")\n",
    "        \n",
    "        \n",
    "        w_i = update_weights(w_i,y_pred,Y_train,alpha,clf)\n",
    "        print(\"SUM\", sum(w_i))\n",
    "\n",
    "\n",
    "def calc_error(weights,Y_train,y_pred):\n",
    "    err = 0\n",
    "    for i in range(len(weights)):\n",
    "        if y_pred[i] != Y_train['y'].iloc[i]:\n",
    "            err= err + weights[i]\n",
    "    # Normalizing the error:\n",
    "    err = err/np.sum(weights)\n",
    "    return err\n",
    "\n",
    "# If the prediction is true, return 0. If it is not true, return 1.\n",
    "def check_pred(y_p, y_t):\n",
    "    if y_p == y_t:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def update_weights(w,y_pred,Y_train,alpha,clf):\n",
    "    for j in range(len(w)):\n",
    "        if y_pred[j] != Y_train['y'].iloc[j]:\n",
    "            w[j] = w[j]* (np.exp( alpha * 1))\n",
    "    return w\n",
    "          \n",
    "def calculate_error_clf(y_pred, y):\n",
    "    sum_error = 0\n",
    "    for i in range(len(y)):\n",
    "        if y_pred[i] != y.iloc[i]['y']:\n",
    "            sum_error += 1\n",
    "        e = (y_pred[i] - y.iloc[i]['y'])**2\n",
    "        \n",
    "        \n",
    "        #sum_error += e\n",
    "    # normalizing:\n",
    "    sum_error = sum_error\n",
    "    return sum_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "WEUGHTS SUM\n",
      "1.0000000000001055\n",
      " \n",
      "HHHEKKKO -8328.0\n",
      "num_of_incorrect 4444\n",
      "0.14860621329947088\n",
      "SUM 1.0741666666665868\n",
      "Iteration: 1\n",
      "WEUGHTS SUM\n",
      "1.0741666666665868\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.14556832342608766\n",
      "SUM 1.1522112211219593\n",
      "Iteration: 2\n",
      "WEUGHTS SUM\n",
      "1.1522112211219593\n",
      " \n",
      "HHHEKKKO -7506.0\n",
      "num_of_incorrect 4353\n",
      "0.2007906019234646\n",
      "SUM 1.2675007308213635\n",
      "Iteration: 3\n",
      "WEUGHTS SUM\n",
      "1.2675007308213635\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15846027178964808\n",
      "SUM 1.3677153773856672\n",
      "Iteration: 4\n",
      "WEUGHTS SUM\n",
      "1.3677153773856672\n",
      " \n",
      "HHHEKKKO -8272.0\n",
      "num_of_incorrect 4450\n",
      "0.1680492133463689\n",
      "SUM 1.4823674312822412\n",
      "Iteration: 5\n",
      "WEUGHTS SUM\n",
      "1.4823674312822412\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.14532267724606446\n",
      "SUM 1.5898890734730062\n",
      "Iteration: 6\n",
      "WEUGHTS SUM\n",
      "1.5898890734730062\n",
      " \n",
      "HHHEKKKO -8472.0\n",
      "num_of_incorrect 4474\n",
      "0.15289395605742573\n",
      "SUM 1.7111950707960426\n",
      "Iteration: 7\n",
      "WEUGHTS SUM\n",
      "1.7111950707960426\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.13558785189802375\n",
      "SUM 1.8270263029776368\n",
      "Iteration: 8\n",
      "WEUGHTS SUM\n",
      "1.8270263029776368\n",
      " \n",
      "HHHEKKKO -8032.0\n",
      "num_of_incorrect 4412\n",
      "0.17917272780860974\n",
      "SUM 1.9902664732514863\n",
      "Iteration: 9\n",
      "WEUGHTS SUM\n",
      "1.9902664732514863\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15090596442851337\n",
      "SUM 2.1401536783858086\n",
      "Iteration: 10\n",
      "WEUGHTS SUM\n",
      "2.1401536783858086\n",
      " \n",
      "HHHEKKKO -7978.0\n",
      "num_of_incorrect 4407\n",
      "0.18245234525569567\n",
      "SUM 2.334851899929765\n",
      "Iteration: 11\n",
      "WEUGHTS SUM\n",
      "2.334851899929765\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15265728622075997\n",
      "SUM 2.5127226826850153\n",
      "Iteration: 12\n",
      "WEUGHTS SUM\n",
      "2.5127226826850153\n",
      " \n",
      "HHHEKKKO -7724.0\n",
      "num_of_incorrect 4384\n",
      "0.19672127734249065\n",
      "SUM 2.7590817098336697\n",
      "Iteration: 13\n",
      "WEUGHTS SUM\n",
      "2.7590817098336697\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.159228157452657\n",
      "SUM 2.9782805304919604\n",
      "Iteration: 14\n",
      "WEUGHTS SUM\n",
      "2.9782805304919604\n",
      " \n",
      "HHHEKKKO -7714.0\n",
      "num_of_incorrect 4385\n",
      "0.19693524693804415\n",
      "SUM 3.2706005804648752\n",
      "Iteration: 15\n",
      "WEUGHTS SUM\n",
      "3.2706005804648752\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15910243602820442\n",
      "SUM 3.5302333847798617\n",
      "Iteration: 16\n",
      "WEUGHTS SUM\n",
      "3.5302333847798617\n",
      " \n",
      "HHHEKKKO -8764.0\n",
      "num_of_incorrect 4538\n",
      "0.13417129788587767\n",
      "SUM 3.7667067405003505\n",
      "Iteration: 17\n",
      "WEUGHTS SUM\n",
      "3.7667067405003505\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.12245289969705758\n",
      "SUM 3.997041077311509\n",
      "Iteration: 18\n",
      "WEUGHTS SUM\n",
      "3.997041077311509\n",
      " \n",
      "HHHEKKKO -8650.0\n",
      "num_of_incorrect 4513\n",
      "0.14075662522148077\n",
      "SUM 4.277882557516783\n",
      "Iteration: 19\n",
      "WEUGHTS SUM\n",
      "4.277882557516783\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.12717735424222087\n",
      "SUM 4.549541396257477\n",
      "Iteration: 20\n",
      "WEUGHTS SUM\n",
      "4.549541396257477\n",
      " \n",
      "HHHEKKKO -8178.0\n",
      "num_of_incorrect 4449\n",
      "0.16988631142088984\n",
      "SUM 4.935067012442929\n",
      "Iteration: 21\n",
      "WEUGHTS SUM\n",
      "4.935067012442929\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.14527080205663923\n",
      "SUM 5.292898510644445\n",
      "Iteration: 22\n",
      "WEUGHTS SUM\n",
      "5.292898510644445\n",
      " \n",
      "HHHEKKKO -7870.0\n",
      "num_of_incorrect 4417\n",
      "0.188457564114196\n",
      "SUM 5.790170990029903\n",
      "Iteration: 23\n",
      "WEUGHTS SUM\n",
      "5.790170990029903\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15511557195348877\n",
      "SUM 6.23834557116504\n",
      "Iteration: 24\n",
      "WEUGHTS SUM\n",
      "6.23834557116504\n",
      " \n",
      "HHHEKKKO -8666.0\n",
      "num_of_incorrect 4529\n",
      "0.13978119781621684\n",
      "SUM 6.673638751767334\n",
      "Iteration: 25\n",
      "WEUGHTS SUM\n",
      "6.673638751767334\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.12624800731861172\n",
      "SUM 7.0943469069787755\n",
      "Iteration: 26\n",
      "WEUGHTS SUM\n",
      "7.0943469069787755\n",
      " \n",
      "HHHEKKKO -7986.0\n",
      "num_of_incorrect 4453\n",
      "0.17868409080394404\n",
      "SUM 7.726489344227281\n",
      "Iteration: 27\n",
      "WEUGHTS SUM\n",
      "7.726489344227281\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.14853740994670658\n",
      "SUM 8.299272961170384\n",
      "Iteration: 28\n",
      "WEUGHTS SUM\n",
      "8.299272961170384\n",
      " \n",
      "HHHEKKKO -7598.0\n",
      "num_of_incorrect 4411\n",
      "0.1914709696619913\n",
      "SUM 9.09138936886006\n",
      "Iteration: 29\n",
      "WEUGHTS SUM\n",
      "9.09138936886006\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.15218022624940422\n",
      "SUM 9.781822260759446\n",
      "Iteration: 30\n",
      "WEUGHTS SUM\n",
      "9.781822260759446\n",
      " \n",
      "HHHEKKKO -7566.0\n",
      "num_of_incorrect 4407\n",
      "0.19338386774739857\n",
      "SUM 10.724708948878948\n",
      "Iteration: 31\n",
      "WEUGHTS SUM\n",
      "10.724708948878948\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.1529709235262229\n",
      "SUM 11.543397437769245\n",
      "Iteration: 32\n",
      "WEUGHTS SUM\n",
      "11.543397437769245\n",
      " \n",
      "HHHEKKKO -8626.0\n",
      "num_of_incorrect 4527\n",
      "0.14341988032637318\n",
      "SUM 12.369757794107803\n",
      "Iteration: 33\n",
      "WEUGHTS SUM\n",
      "12.369757794107803\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.1288402979287038\n",
      "SUM 13.165518947136741\n",
      "Iteration: 34\n",
      "WEUGHTS SUM\n",
      "13.165518947136741\n",
      " \n",
      "HHHEKKKO -8622.0\n",
      "num_of_incorrect 4537\n",
      "0.14292534934051593\n",
      "SUM 14.104763807743844\n",
      "Iteration: 35\n",
      "WEUGHTS SUM\n",
      "14.104763807743844\n",
      " \n",
      "HHHEKKKO 9600.0\n",
      "num_of_incorrect 4762\n",
      "0.128076720317965\n",
      "SUM 15.006777063859694\n",
      "Iteration: 36\n",
      "WEUGHTS SUM\n",
      "15.006777063859694\n",
      " \n",
      "HHHEKKKO -8232.0\n"
     ]
    }
   ],
   "source": [
    "adaBoost(X_train, Y_train,tree.DecisionTreeClassifier(max_depth = 1))\n",
    "print(\" \")\n",
    "print(\"THIS IS THE FINAL ANSWER:\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [  1.07416667   1.15221122   1.26750073   1.36771538   1.48236743\n",
    "   1.58988907   1.71119507   1.8270263    1.99026647   2.14015368\n",
    "   2.3348519    2.51272268   2.75908171   2.97828053   3.27060058\n",
    "   3.53023338   3.76670674   3.99704108   4.27788256   4.5495414\n",
    "   4.93506701   5.29289851   5.79017099   6.23834557   6.67363875\n",
    "   7.09434691   7.72648934   8.29927296   9.09138937   9.78182226\n",
    "  10.72470895  11.54339744  12.36975779  13.16551895  14.10476381\n",
    "  15.00677706  16.23821602  17.38139213  18.70497891  19.94586914\n",
    "  21.75535205  23.3540812   25.13934856  26.80759423  28.69769378\n",
    "  30.49131412  33.14177918  35.50844746  38.19825555  40.71155195\n",
    "  44.06033217  47.10567442  51.52476383  55.25651025  58.67638797\n",
    "  61.98180932  65.63103277  69.18083788  74.23875745  78.96365097\n",
    "  85.73631986  91.71727522  97.79981657 103.59674021 109.85807466\n",
    " 115.89414937 122.90792113 129.66669215 136.90227918 143.94316645\n",
    " 153.12727021 161.87938238 171.18662168 180.18191721 190.31974746\n",
    " 200.14045327 216.51019633 230.9360824  247.52801389 262.80507311\n",
    " 277.97670703 292.5953711  308.69230008 324.31681619 348.93585357\n",
    " 371.21135875 399.10824474 424.31911024 444.81477697 464.98234178\n",
    " 489.3903222  513.17904405 536.94524912 562.74096628 608.63369284\n",
    " 635.16203193 661.40960691 696.81297728 731.12258153 769.79711472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_weights(w,X_train,Y_train,alpha,clf):\n",
    "  #   for j in range(len(w)):\n",
    "  #      w[j] = w[j] * np.exp(alpha*check_pred(classifier, X_train[j], Y_train['y'].iloc[j]))\n",
    "  #   return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating G(x):\n",
    "    # result = 0\n",
    "    # for c in classifiers:\n",
    "        # result += c[2]* c[0](x)\n",
    "    # res = np.sign(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
