{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV:\n",
    "\n",
    "def loadCSV(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', skiprows=0, unpack=False)\n",
    "    # xarray = data[...,0] # First row! Need to get this on a matrix-form!\n",
    "    X = data[:,range(data.shape[1]-1)] # Kolonne 1 og 2 kommer ut for 2D\n",
    "    numOfColumns = data.shape[1]\n",
    "    Y = data[:,data.shape[1]-1]\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.c_[ones,X] # Merge two matrices\n",
    "    return data, X, Y\n",
    "\n",
    "dataTrain, XTrain, YTrain = loadCSV('cl_train_1.csv')\n",
    "dataTest, xarrayTest, YTest = loadCSV('cl_test_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with Gradient descent. Defining functions:\n",
    "# z = h(x)\n",
    "def linearSignal(w, x):\n",
    "    return (w.T).dot(x)\n",
    "\n",
    "# Sigma, the sigmoid function. Returns a number between 0 and 1. \n",
    "def logistic(z):\n",
    "    return ( 1 / (1 + math.exp(-z)))\n",
    "    \n",
    "# Returns a probability that the datapoint (1,x1,x2) is in y.\n",
    "def prob(w,x,y):\n",
    "    z = linearSignal(w,x)\n",
    "    probability = (logistic(z)**y)*(1 - logistic(z))**(1-y)\n",
    "    return probability\n",
    "\n",
    "# Calculating the cross-entropy error on the training and the test-set:\n",
    "def CEE(X,Y,trainedWeights):\n",
    "    error = 0\n",
    "    place = 0\n",
    "    for y in Y:\n",
    "        z = linearSignal(trainedWeights,X[place])\n",
    "        sigmaZ = logistic(z)\n",
    "        error = error + (y* (np.log(sigmaZ)) + (1 - y)*np.log(1-sigmaZ))\n",
    "        place += 1\n",
    "    return -error/((Y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculation the weights with Gradient descent:\n",
    "\n",
    "def gradientDescent(X,Y,LR,weights,iterations):\n",
    "    it = 0\n",
    "    error_array = np.ones(iterations)\n",
    "    placeInErrorArray = 0\n",
    "    while it < iterations:\n",
    "        place = 0\n",
    "        summ = 0\n",
    "        \n",
    "        \n",
    "        for x in X:\n",
    "            summ = summ + np.dot((logistic(linearSignal(weights,x)) - Y[place]),x)\n",
    "            place+=1\n",
    "            \n",
    "        # Calculating the error with CEE for each iteration:\n",
    "        error = CEE(X, Y, weights)\n",
    "        \n",
    "        # Adding it to the error_array:\n",
    "        error_array[placeInErrorArray] = error\n",
    "        \n",
    "        # Calc the weights for every iteration:\n",
    "        weights = weights - (LR*summ)\n",
    "        it+=1\n",
    "        placeInErrorArray += 1\n",
    "        \n",
    "    return weights, error_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights:  [ 12.20510561 -34.23846311  17.66821299]\n",
      "Error-array [0.75712364 1.1378614  2.34724094 0.74706378 1.55483987 0.9896409\n",
      " 1.3542974  0.74866317 0.79017898 0.6133923  0.51253651 0.41614888\n",
      " 0.32423411 0.27153045 0.2165233  0.18766708 0.16132461 0.14743265\n",
      " 0.13690317 0.13083251 0.12648517 0.12342806 0.12096457 0.11886116\n",
      " 0.1169592  0.11519257 0.11352581 0.11194125 0.11042812 0.10897911\n",
      " 0.10758849 0.1062515  0.104964   0.1037224  0.10252347 0.10136438\n",
      " 0.10024257 0.09915573 0.0981018  0.0970789  0.09608532 0.0951195\n",
      " 0.09418001 0.09326553 0.09237487 0.09150691 0.0906606  0.089835\n",
      " 0.0890292  0.08824237 0.08747374 0.08672257 0.08598817 0.0852699\n",
      " 0.08456715 0.08387936 0.08320597 0.08254649 0.08190041 0.08126729\n",
      " 0.08064669 0.08003818 0.07944139 0.07885592 0.07828143 0.07771757\n",
      " 0.07716402 0.07662046 0.07608659 0.07556213 0.0750468  0.07454035\n",
      " 0.07404252 0.07355307 0.07307176 0.07259839 0.07213272 0.07167456\n",
      " 0.0712237  0.07077996 0.07034316 0.0699131  0.06948964 0.06907259\n",
      " 0.0686618  0.06825712 0.06785841 0.0674655  0.06707828 0.0666966\n",
      " 0.06632033 0.06594935 0.06558354 0.06522279 0.06486697 0.06451598\n",
      " 0.06416971 0.06382805 0.06349092 0.0631582  0.06282982 0.06250566\n",
      " 0.06218566 0.06186971 0.06155774 0.06124967 0.06094541 0.06064489\n",
      " 0.06034803 0.06005477 0.05976503 0.05947874 0.05919583 0.05891625\n",
      " 0.05863993 0.0583668  0.05809681 0.0578299  0.05756601 0.05730508\n",
      " 0.05704706 0.05679191 0.05653956 0.05628996 0.05604308 0.05579886\n",
      " 0.05555725 0.05531821 0.0550817  0.05484766 0.05461607 0.05438687\n",
      " 0.05416003 0.05393551 0.05371327 0.05349328 0.05327549 0.05305987\n",
      " 0.05284638 0.052635   0.05242568 0.0522184  0.05201312 0.05180981\n",
      " 0.05160844 0.05140898 0.05121141 0.05101568 0.05082178 0.05062968\n",
      " 0.05043935 0.05025076 0.05006389 0.04987871 0.0496952  0.04951334\n",
      " 0.04933309 0.04915444 0.04897736 0.04880183 0.04862783 0.04845534\n",
      " 0.04828434 0.0481148  0.04794671 0.04778004 0.04761478 0.0474509\n",
      " 0.04728839 0.04712723 0.04696741 0.04680889 0.04665167 0.04649573\n",
      " 0.04634106 0.04618763 0.04603542 0.04588444 0.04573464 0.04558604\n",
      " 0.0454386  0.04529231 0.04514716 0.04500313 0.04486021 0.04471839\n",
      " 0.04457765 0.04443798 0.04429937 0.0441618  0.04402525 0.04388973\n",
      " 0.04375521 0.04362169 0.04348915 0.04335757 0.04322696 0.04309729\n",
      " 0.04296856 0.04284076 0.04271386 0.04258788 0.04246278 0.04233857\n",
      " 0.04221524 0.04209276 0.04197114 0.04185037 0.04173042 0.04161131\n",
      " 0.04149301 0.04137552 0.04125882 0.04114291 0.04102779 0.04091344\n",
      " 0.04079985 0.04068702 0.04057493 0.04046359 0.04035297 0.04024308\n",
      " 0.04013391 0.04002545 0.03991768 0.03981061 0.03970423 0.03959853\n",
      " 0.0394935  0.03938914 0.03928543 0.03918238 0.03907998 0.03897821\n",
      " 0.03887708 0.03877657 0.03867668 0.03857741 0.03847874 0.03838068\n",
      " 0.03828321 0.03818633 0.03809004 0.03799433 0.03789919 0.03780461\n",
      " 0.0377106  0.03761715 0.03752425 0.03743189 0.03734008 0.0372488\n",
      " 0.03715805 0.03706783 0.03697812 0.03688894 0.03680027 0.0367121\n",
      " 0.03662443 0.03653727 0.03645059 0.03636441 0.0362787  0.03619348\n",
      " 0.03610873 0.03602446 0.03594065 0.0358573  0.03577441 0.03569198\n",
      " 0.03561    0.03552846 0.03544737 0.03536671 0.03528649 0.0352067\n",
      " 0.03512734 0.0350484  0.03496988 0.03489178 0.03481409 0.03473682\n",
      " 0.03465994 0.03458347 0.0345074  0.03443172 0.03435644 0.03428155\n",
      " 0.03420704 0.03413291 0.03405917 0.0339858  0.0339128  0.03384018\n",
      " 0.03376792 0.03369603 0.03362449 0.03355332 0.0334825  0.03341204\n",
      " 0.03334192 0.03327215 0.03320273 0.03313365 0.0330649  0.0329965\n",
      " 0.03292842 0.03286068 0.03279326 0.03272618 0.03265941 0.03259297\n",
      " 0.03252684 0.03246103 0.03239553 0.03233035 0.03226547 0.0322009\n",
      " 0.03213663 0.03207267 0.032009   0.03194564 0.03188256 0.03181978\n",
      " 0.03175729 0.03169509 0.03163318 0.03157155 0.0315102  0.03144913\n",
      " 0.03138834 0.03132782 0.03126758 0.03120761 0.03114791 0.03108848\n",
      " 0.03102932 0.03097042 0.03091178 0.0308534  0.03079528 0.03073742\n",
      " 0.03067981 0.03062246 0.03056535 0.0305085  0.03045189 0.03039553\n",
      " 0.03033942 0.03028354 0.03022791 0.03017252 0.03011736 0.03006244\n",
      " 0.03000776 0.02995331 0.02989909 0.0298451  0.02979133 0.0297378\n",
      " 0.02968448 0.0296314  0.02957853 0.02952588 0.02947346 0.02942125\n",
      " 0.02936926 0.02931748 0.02926591 0.02921456 0.02916342 0.02911248\n",
      " 0.02906176 0.02901124 0.02896092 0.02891081 0.0288609  0.0288112\n",
      " 0.02876169 0.02871238 0.02866327 0.02861435 0.02856563 0.0285171\n",
      " 0.02846877 0.02842062 0.02837266 0.0283249  0.02827732 0.02822993\n",
      " 0.02818272 0.02813569 0.02808885 0.02804219 0.02799571 0.02794941\n",
      " 0.02790329 0.02785735 0.02781158 0.02776599 0.02772057 0.02767532\n",
      " 0.02763024 0.02758534 0.0275406  0.02749604 0.02745164 0.02740741\n",
      " 0.02736334 0.02731944 0.0272757  0.02723213 0.02718872 0.02714546\n",
      " 0.02710237 0.02705944 0.02701666 0.02697404 0.02693158 0.02688927\n",
      " 0.02684712 0.02680512 0.02676327 0.02672158 0.02668003 0.02663864\n",
      " 0.02659739 0.0265563  0.02651534 0.02647454 0.02643388 0.02639337\n",
      " 0.026353   0.02631277 0.02627269 0.02623274 0.02619294 0.02615328\n",
      " 0.02611375 0.02607437 0.02603512 0.02599601 0.02595704 0.0259182\n",
      " 0.02587949 0.02584092 0.02580248 0.02576418 0.02572601 0.02568796\n",
      " 0.02565005 0.02561227 0.02557461 0.02553708 0.02549968 0.02546241\n",
      " 0.02542527 0.02538824 0.02535135 0.02531457 0.02527792 0.0252414\n",
      " 0.02520499 0.02516871 0.02513255 0.0250965  0.02506058 0.02502477\n",
      " 0.02498909 0.02495352 0.02491806 0.02488273 0.02484751 0.0248124\n",
      " 0.02477741 0.02474253 0.02470777 0.02467312 0.02463858 0.02460415\n",
      " 0.02456984 0.02453563 0.02450153 0.02446755 0.02443367 0.0243999\n",
      " 0.02436624 0.02433268 0.02429923 0.02426589 0.02423265 0.02419952\n",
      " 0.02416649 0.02413356 0.02410074 0.02406802 0.02403541 0.02400289\n",
      " 0.02397048 0.02393817 0.02390595 0.02387384 0.02384183 0.02380991\n",
      " 0.0237781  0.02374638 0.02371476 0.02368323 0.02365181 0.02362047\n",
      " 0.02358924 0.0235581  0.02352705 0.0234961  0.02346524 0.02343447\n",
      " 0.0234038  0.02337322 0.02334273 0.02331233 0.02328202 0.02325181\n",
      " 0.02322168 0.02319165 0.0231617  0.02313184 0.02310207 0.02307239\n",
      " 0.02304279 0.02301329 0.02298387 0.02295453 0.02292528 0.02289612\n",
      " 0.02286704 0.02283805 0.02280914 0.02278032 0.02275157 0.02272292\n",
      " 0.02269434 0.02266585 0.02263744 0.02260911 0.02258086 0.02255269\n",
      " 0.02252461 0.0224966  0.02246867 0.02244083 0.02241306 0.02238537\n",
      " 0.02235776 0.02233023 0.02230277 0.0222754  0.02224809 0.02222087\n",
      " 0.02219372 0.02216665 0.02213965 0.02211273 0.02208589 0.02205912\n",
      " 0.02203242 0.0220058  0.02197925 0.02195277 0.02192637 0.02190004\n",
      " 0.02187378 0.02184759 0.02182148 0.02179543 0.02176946 0.02174356\n",
      " 0.02171773 0.02169197 0.02166628 0.02164066 0.02161511 0.02158962\n",
      " 0.02156421 0.02153886 0.02151358 0.02148837 0.02146323 0.02143816\n",
      " 0.02141315 0.02138821 0.02136333 0.02133852 0.02131378 0.0212891\n",
      " 0.02126449 0.02123994 0.02121546 0.02119104 0.02116668 0.02114239\n",
      " 0.02111816 0.021094   0.0210699  0.02104586 0.02102189 0.02099797\n",
      " 0.02097412 0.02095033 0.02092661 0.02090294 0.02087934 0.02085579\n",
      " 0.02083231 0.02080888 0.02078552 0.02076222 0.02073897 0.02071579\n",
      " 0.02069266 0.0206696  0.02064659 0.02062364 0.02060075 0.02057791\n",
      " 0.02055514 0.02053242 0.02050976 0.02048715 0.0204646  0.02044211\n",
      " 0.02041968 0.0203973  0.02037498 0.02035271 0.0203305  0.02030834\n",
      " 0.02028624 0.02026419 0.0202422  0.02022026 0.02019837 0.02017654\n",
      " 0.02015477 0.02013304 0.02011137 0.02008976 0.02006819 0.02004668\n",
      " 0.02002522 0.02000382 0.01998246 0.01996116 0.01993991 0.01991871\n",
      " 0.01989756 0.01987646 0.01985542 0.01983442 0.01981347 0.01979258\n",
      " 0.01977173 0.01975094 0.01973019 0.0197095  0.01968885 0.01966825\n",
      " 0.01964771 0.01962721 0.01960675 0.01958635 0.019566   0.01954569\n",
      " 0.01952543 0.01950522 0.01948506 0.01946494 0.01944487 0.01942485\n",
      " 0.01940487 0.01938495 0.01936506 0.01934523 0.01932544 0.01930569\n",
      " 0.019286   0.01926634 0.01924674 0.01922717 0.01920766 0.01918819\n",
      " 0.01916876 0.01914938 0.01913004 0.01911074 0.01909149 0.01907229\n",
      " 0.01905313 0.01903401 0.01901493 0.0189959  0.01897691 0.01895797\n",
      " 0.01893907 0.01892021 0.01890139 0.01888262 0.01886388 0.01884519\n",
      " 0.01882654 0.01880794 0.01878937 0.01877085 0.01875237 0.01873393\n",
      " 0.01871553 0.01869717 0.01867885 0.01866057 0.01864234 0.01862414\n",
      " 0.01860598 0.01858787 0.01856979 0.01855175 0.01853376 0.0185158\n",
      " 0.01849788 0.01848    0.01846216 0.01844436 0.0184266  0.01840888\n",
      " 0.01839119 0.01837355 0.01835594 0.01833837 0.01832084 0.01830334\n",
      " 0.01828589 0.01826847 0.01825109 0.01823374 0.01821644 0.01819917\n",
      " 0.01818194 0.01816474 0.01814758 0.01813046 0.01811338 0.01809633\n",
      " 0.01807932 0.01806234 0.0180454  0.01802849 0.01801162 0.01799479\n",
      " 0.01797799 0.01796123 0.0179445  0.01792781 0.01791116 0.01789453\n",
      " 0.01787795 0.0178614  0.01784488 0.01782839 0.01781195 0.01779553\n",
      " 0.01777915 0.01776281 0.01774649 0.01773021 0.01771397 0.01769776\n",
      " 0.01768158 0.01766544 0.01764933 0.01763325 0.0176172  0.01760119\n",
      " 0.01758521 0.01756926 0.01755335 0.01753747 0.01752162 0.0175058\n",
      " 0.01749002 0.01747427 0.01745855 0.01744286 0.0174272  0.01741158\n",
      " 0.01739598 0.01738042 0.01736489 0.01734939 0.01733392 0.01731849\n",
      " 0.01730308 0.01728771 0.01727236 0.01725705 0.01724177 0.01722651\n",
      " 0.01721129 0.0171961  0.01718094 0.01716581 0.0171507  0.01713563\n",
      " 0.01712059 0.01710558 0.0170906  0.01707564 0.01706072 0.01704583\n",
      " 0.01703096 0.01701613 0.01700132 0.01698654 0.01697179 0.01695707\n",
      " 0.01694238 0.01692772 0.01691309 0.01689848 0.01688391 0.01686936\n",
      " 0.01685484 0.01684034 0.01682588 0.01681144 0.01679703 0.01678265\n",
      " 0.0167683  0.01675398 0.01673968 0.01672541 0.01671116 0.01669695\n",
      " 0.01668276 0.0166686  0.01665447 0.01664036 0.01662628 0.01661222\n",
      " 0.0165982  0.0165842  0.01657022 0.01655627 0.01654235 0.01652846\n",
      " 0.01651459 0.01650075 0.01648693 0.01647314 0.01645938 0.01644564\n",
      " 0.01643193 0.01641824 0.01640458 0.01639094 0.01637733 0.01636375\n",
      " 0.01635019 0.01633665 0.01632314 0.01630966 0.0162962  0.01628276\n",
      " 0.01626936 0.01625597 0.01624261 0.01622928 0.01621596 0.01620268\n",
      " 0.01618942 0.01617618 0.01616297 0.01614978 0.01613661 0.01612347\n",
      " 0.01611036 0.01609726 0.01608419 0.01607115 0.01605813 0.01604513\n",
      " 0.01603216 0.01601921 0.01600628 0.01599338 0.0159805  0.01596764\n",
      " 0.0159548  0.01594199 0.01592921 0.01591644 0.0159037  0.01589098\n",
      " 0.01587829 0.01586561 0.01585296 0.01584033 0.01582773 0.01581515\n",
      " 0.01580259 0.01579005 0.01577753 0.01576504 0.01575257 0.01574012\n",
      " 0.01572769 0.01571529 0.0157029  0.01569054 0.0156782  0.01566588\n",
      " 0.01565359 0.01564131 0.01562906 0.01561683 0.01560462 0.01559243\n",
      " 0.01558026 0.01556812 0.01555599 0.01554389 0.01553181 0.01551974\n",
      " 0.0155077  0.01549568 0.01548368 0.01547171 0.01545975 0.01544781\n",
      " 0.0154359  0.015424   0.01541213 0.01540027 0.01538844 0.01537662\n",
      " 0.01536483 0.01535306 0.01534131 0.01532957 0.01531786 0.01530617\n",
      " 0.0152945  0.01528284 0.01527121 0.0152596  0.015248   0.01523643\n",
      " 0.01522488 0.01521334 0.01520183 0.01519033 0.01517886 0.0151674\n",
      " 0.01515596 0.01514454 0.01513314 0.01512177 0.01511041 0.01509906\n",
      " 0.01508774 0.01507644 0.01506515 0.01505389 0.01504264 0.01503141\n",
      " 0.01502021 0.01500902 0.01499784 0.01498669 0.01497556 0.01496444\n",
      " 0.01495334 0.01494226 0.0149312  0.01492016 0.01490913 0.01489813\n",
      " 0.01488714 0.01487617 0.01486522 0.01485428 0.01484337 0.01483247\n",
      " 0.01482159 0.01481073 0.01479988 0.01478906 0.01477825 0.01476746\n",
      " 0.01475668 0.01474593 0.01473519 0.01472447]\n",
      "Training-error:  0.014703076083241945\n"
     ]
    }
   ],
   "source": [
    "# Using the functions to calculate values:\n",
    "\n",
    "# Setting the initial weights in the range -0.5 to 0.5:\n",
    "initWeights = np.array([0.5, 0.5,0.5])\n",
    "\n",
    "\n",
    "trained_Weights, error_array = gradientDescent(XTrain,YTrain, 0.2,initWeights, 1000)\n",
    "\n",
    "print(\"Trained weights: \", trained_Weights)\n",
    "print(\"Error-array\", error_array)\n",
    "\n",
    "error1 = CEE(XTrain, YTrain, trained_Weights6)\n",
    "print(\"Training-error: \", error1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
