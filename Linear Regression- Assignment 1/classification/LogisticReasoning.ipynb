{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV:\n",
    "\n",
    "def loadCSV(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', skiprows=1, unpack=False)\n",
    "    xarray = data[...,0] # Need to get this on a matrix-form!\n",
    "    numOfColumns = data.shape[1]\n",
    "    Y = data[...,(numOfColumns-1)]\n",
    "    return data, xarray, Y\n",
    "    \n",
    "dataTrain, xarrayTrain, YTrain = loadCSV('cl_train_1.csv')\n",
    "dataTest, xarrayTest, YTest = loadCSV('cl_test_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place 1 in position 0 in each x-array in the matrix X:\n",
    "\n",
    "def addOnes(data,xArrayWithoutOnes):\n",
    "    X = np.ones((len(xArrayWithoutOnes), (data.shape[1])))\n",
    "    place = 0\n",
    "    \n",
    "    #1D\n",
    "    if data.shape[1] == 2:\n",
    "        for obs in xArrayWithoutOnes:\n",
    "            # Put one obs in each place in X\n",
    "            X[place,1] = obs\n",
    "            place+=1\n",
    "    #2D\n",
    "    if data.shape[1] == 3:\n",
    "        place = 0\n",
    "        for obs in data:\n",
    "            # Put one obs in each place in X\n",
    "            X[place,1]= obs[1]\n",
    "            X[place,2] = obs[2]\n",
    "            place+=1\n",
    "    return X\n",
    "\n",
    "XTrain = addOnes(dataTrain,xarrayTrain)\n",
    "XTest = addOnes(dataTest, xarrayTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with Gradient descent. Defining functions:\n",
    "\n",
    "# z = h(x)\n",
    "def linearSignal(w, x):\n",
    "    return (w.T).dot(x)\n",
    "\n",
    "# Sigma, the sigmoid function. Returns a number between 0 and 1. \n",
    "def logistic(z):\n",
    "    return ( 1 / (1 + math.exp(-z)))\n",
    "    \n",
    "# Returns a probability that the datapoint (1,x1,x2) is in y.\n",
    "def prob(w,x,y):\n",
    "    z = linearSignal(w,x)\n",
    "    probability = (logistic(z)**y)*(1 - logistic(z))**(1-y)\n",
    "    return probability\n",
    "\n",
    "# Calculating the cross-entropy error on the training and the test-set:\n",
    "def CEE(w):\n",
    "    logistic()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.16536558  2.45810008 21.1851329 ]\n",
      "[-7.98095304  1.52235986 21.18583049]\n",
      "[-9.80644981  0.70143448 21.25066209]\n",
      "[-10.79382799   0.41640576  22.00513745]\n",
      "[-11.05452221   0.39757645  22.50092381]\n",
      "[-11.19171058   0.39086856  22.77667666]\n",
      "[-11.31135966   0.38568705  23.01944366]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent:\n",
    "\n",
    "def gradientDescent(X,Y,LR,weights,iterations):\n",
    "    it = 0\n",
    "    while it <= iterations:\n",
    "        \n",
    "        place = 0\n",
    "        summ = 0\n",
    "        \n",
    "        for x in X:\n",
    "\n",
    "            summ = summ + (logistic(linearSignal(weights,x)) - Y[place])*x\n",
    "            place+=1\n",
    "            \n",
    "        weights = weights - (LR*summ)\n",
    "        it+=1\n",
    "    return weights\n",
    "\n",
    "# Setting the initial weights in the range -0.5 to 0.5:\n",
    "initWeights = np.array([0.2, 0.2,0.2])\n",
    "\n",
    "# learning rate in the range -0.5 to 0.5:\n",
    "trained_Weights1 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 10)\n",
    "trained_Weights2 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 100)\n",
    "trained_Weights3 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 1000)\n",
    "trained_Weights4 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 5000)\n",
    "trained_Weights5 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 8000)\n",
    "trained_Weights6 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 10000)\n",
    "trained_Weights7 = gradientDescent(XTrain,YTrain, 0.5,initWeights, 12000)\n",
    "\n",
    "\n",
    "print(trained_Weights1)\n",
    "print(trained_Weights2)\n",
    "print(trained_Weights3)\n",
    "print(trained_Weights4)\n",
    "print(trained_Weights5)\n",
    "print(trained_Weights6)\n",
    "print(trained_Weights7)\n",
    "\n",
    "# Looks like it is converging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
